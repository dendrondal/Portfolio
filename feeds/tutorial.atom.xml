<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Dal's Portfolio - tutorial</title><link href="http://dalwilliams.com/" rel="alternate"></link><link href="http://dalwilliams.com/feeds/tutorial.atom.xml" rel="self"></link><id>http://dalwilliams.com/</id><updated>2020-10-16T00:00:00-05:00</updated><entry><title>Packaging with Poetry and Pipx!</title><link href="http://dalwilliams.com/packaging-with-poetry-and-pipx.html" rel="alternate"></link><published>2020-10-16T00:00:00-05:00</published><updated>2020-10-16T00:00:00-05:00</updated><author><name>Jon Steven Dal Williams</name></author><id>tag:dalwilliams.com,2020-10-16:/packaging-with-poetry-and-pipx.html</id><summary type="html">&lt;p&gt;Ask most Python devs the largest shortcoming of the language, and they'll most likely say one of two things: the lack of &lt;a href="https://talkpython.fm/episodes/show/245/python-packaging-landscape-in-2020"&gt;mobile presence&lt;/a&gt; or the difficulties with packaging. It's easy to be jealous of Go/Rust programmers' ability to package everything into a binary install, or C# devs putting …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ask most Python devs the largest shortcoming of the language, and they'll most likely say one of two things: the lack of &lt;a href="https://talkpython.fm/episodes/show/245/python-packaging-landscape-in-2020"&gt;mobile presence&lt;/a&gt; or the difficulties with packaging. It's easy to be jealous of Go/Rust programmers' ability to package everything into a binary install, or C# devs putting their app into an exe. Python may not have this luxury, but that doesn't negate its ability to make clean, intuitive CLIs in a short period of time.&lt;/p&gt;
&lt;p&gt;The versioning difficulties are compounded when you try to share your creation with a non-technical audience. It's unreasonable to expect your end-users to have modern Python installed, or to even know what that is in the first place. However, even scientists who have never coded in their life probably have experience with scripting via Excel, LabView, Origin, Igor, or many other macro-based apps. As I couldn't find a tutorial on this exact topic when packaging several tools I wrote for my graduate work, I decided to write one for future devs/scientists trying to do the same.&lt;/p&gt;
&lt;section id="two-solutions"&gt;
&lt;h2&gt;Two solutions&lt;/h2&gt;
&lt;p&gt;This can be solved by either a lightweight or more versatile heavyweight approach. The lightweight approach involves &lt;a href="https://pipxproject.github.io/pipx/"&gt;pipx&lt;/a&gt;, a utility that isolates a CLI and its requirements in a venv and adds them to the &lt;cite&gt;PATH&lt;/cite&gt; automatically. It's pretty great at what it does, and I've found it to be the most pain-free way to install things like Black or Poetry globally without breaking any dependencies or builds.&lt;/p&gt;
&lt;p&gt;Speaking of &lt;a href="https://python-poetry.org/"&gt;Poetry&lt;/a&gt;, it is actually vital to both of these solutions. For those unfamiliar, Poetry is an all-in-one dependency management, venv, and packaging utility. It uses the rather friendly TOML language to define versions in a fuzzy manner, and will basically refuse to install anything if there's a version mismatch, e.g. one app requires python 3.6.4+, but the project only requires 3.6.0+. These requirements are also locked in place, so people can add dependencies asynchronously without worrying about breaking the build, because poetry will throw a ton of errors your way. It also works very nicely with &lt;a href="https://github.com/pyenv/pyenv"&gt;pyenv&lt;/a&gt;, and allows you to export your requirements to the classic &lt;cite&gt;requirements.txt&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;The latter is important because of the heavyweight solution: Docker. Pipx installs dependencies via pip. If you have non-pip dependencies, Docker is likely the way to go. An example in my recent past is a project that required a LaTeX install, which can be &lt;a href="https://dalwilliams.info/lessons-learned-from-writing-a-phd-dissertation-in-markdown.html"&gt;quite painful&lt;/a&gt; and is prone to OS-dependent errors. So, I wrapped everything up in a &lt;a href="https://github.com/dendrondal/phd_thesis_markdown"&gt;Docker container&lt;/a&gt;, and everything worked perfectly. However, this takes quite a bit more overhead on the end-users part, and requires more space to be used. Docker is complex enough that it would require another tutorial, but thankfully there are already many &lt;a href="https://pythonspeed.com/"&gt;good ones&lt;/a&gt; out there.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="packaging-testing"&gt;
&lt;h2&gt;Packaging &amp;amp; Testing&lt;/h2&gt;
&lt;p&gt;Now that we know our options, let's go through how to get this up and working. I'll be using the actual project that inspired this post, &lt;a href="https://github.com/dendrondal/instrumentools"&gt;instrumentools&lt;/a&gt;, as an example, but this approach should be relatively agnostic. That is, whether you use &lt;a href="https://realpython.com/command-line-interfaces-python-argparse/"&gt;argparse&lt;/a&gt;, &lt;a href="https://click.palletsprojects.com/en/7.x/"&gt;click&lt;/a&gt;, or &lt;a href="https://typer.tiangolo.com/"&gt;typer&lt;/a&gt; to create your CLI, these steps should serve you regardless. Let's first go through a basic idea of how to structure this project:&lt;/p&gt;
&lt;pre&gt;.
|── poetry.lock
|── pyproject.toml
|── README.md
|── instrumentools
|   └── __init__.py
|       CAC.py
|       TEM.py
|       thermal_analysis.py&lt;/pre&gt;
&lt;p&gt;The key factor is that each of your CLI components are stored in modules within the parent directory. This allows you to define them as entrypoints within your &lt;cite&gt;pyproject.toml&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="k"&gt;[tool.poetry.scripts]&lt;/span&gt;
&lt;span class="n"&gt;tem_analysis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;instrumentools.TEM:main&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;cac_analysis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;instrumentools.CAC:cac_graphing&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;thermal_analysis&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;instrumentools.thermal_analysis:main&amp;quot;&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;This creates symlinks to Python executing the &lt;cite&gt;main&lt;/cite&gt; function in the &lt;cite&gt;cli.py&lt;/cite&gt; file of each module. To test whether this worked, you first need to run &lt;cite&gt;poetry install&lt;/cite&gt; to set up the links, and run &lt;cite&gt;poetry run FEATURE&lt;/cite&gt;, where feature could be tem_analysis, cac_analysis, or thermal_analysis in the case above. Alternatively, you could use click to allow multiple entrypoints via a &lt;a href="https://click.palletsprojects.com/en/7.x/commands/"&gt;single command line script&lt;/a&gt;, which could be better in terms of namespace pollution. I'm sure there are also ways to do this with the other CLI software listed above.&lt;/p&gt;
&lt;aside class="m-block m-primary"&gt;
&lt;h3&gt;Ideally, it would make more sense to include each feature as a package rather than a module, making your project more modular and less cluttered. Unfortunately, I have not found a way to do this with Poetry's scripts command in a way that pipx understands.&lt;/h3&gt;
&lt;/aside&gt;
&lt;p&gt;As mentioned before, poetry makes packaging very easy. First, you should create an account on &lt;a href="https://test.pypi.org/"&gt;test.pypi.org&lt;/a&gt;. Just to make things easier and safer, I would recommend going to account settings and getting an API key. Back on the command line, do this:&lt;/p&gt;
&lt;pre class="m-code"&gt;$ poetry config respositories.testpypi.url https://test.pypi.org/legacy/
$ poetry config pypi-token.testpypi &amp;lt;YOUR_API_KEY&amp;gt;
$ poetry publish -r testpypi --build&lt;/pre&gt;
&lt;p&gt;As of Poetry 1.0, having the legacy prefix for testpypi is important. You can view your configuration settings using &lt;cite&gt;poetry config --list&lt;/cite&gt;, but your api key will be hidden. After running the final command, your package should show up on your testpypi account. You will generate both zipped source code and a &lt;a href="https://realpython.com/python-wheels/"&gt;wheel&lt;/a&gt; in the newly created &lt;cite&gt;dist&lt;/cite&gt; folder within your repository. There are multiple ways to test whether or not this worked. Note that pipx takes awhile to sync with your remote repository, so the local version may work better in this case:&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;# Local installation&lt;/span&gt;
pipx run --spec PATH_TO_YOUR_PROJECT/dist/VERSION.whl FEATURE

&lt;span class="c1"&gt;# Remote install&lt;/span&gt;
pipx run WHEEL_URL FEATURE&lt;/pre&gt;
&lt;p&gt;Here, your &lt;cite&gt;WHEEL_URL&lt;/cite&gt; can be found under the link for the current release, in the releases page of your project on testpypi. Registering your project to PyPi goes through a very similar process to the test server, just with pypi.org instead, so you can effectively repeat the steps above once you're comfortable with the result. Then by running &lt;cite&gt;pipx install instrumentools&lt;/cite&gt;, every command works as expected. As with any programming endeavor, make sure that your code is well-documented! It's important that the end-user doesn't need to memorize commands and can get a nice set of instructions by using &lt;cite&gt;--help&lt;/cite&gt;:&lt;/p&gt;
&lt;pre class="m-code"&gt;Usage: cac_analysis &lt;span class="o"&gt;[&lt;/span&gt;OPTIONS&lt;span class="o"&gt;]&lt;/span&gt;

Graphs .csv output from Bruker UV-Vis software. Outputs stacked UV-vis
spectra and wavelength &lt;span class="o"&gt;(&lt;/span&gt;or wavelength raio, depending on dye&lt;span class="o"&gt;)&lt;/span&gt; vs.
log&lt;span class="o"&gt;(&lt;/span&gt;concentration&lt;span class="o"&gt;)&lt;/span&gt; spectra as .png files in the same directory as .csv
file.

Options:
  --path PATH      Full path to .csv file
  --wv_range TEXT  Range of wavelengths &lt;span class="o"&gt;(&lt;/span&gt;i.e. max - min&lt;span class="o"&gt;)&lt;/span&gt;
  --min_conc TEXT  Minimum concentration, in mg/mL
  --max_conc TEXT  Maximum concentration, in mg/mL
  --step TEXT      Total number of samples in csv
  --vb1 TEXT       Lambda max/first vibronic band
  --vb3 TEXT       If you are comparing the first and third vibronic band &lt;span class="k"&gt;for&lt;/span&gt;
                   a dye, &lt;span class="o"&gt;(&lt;/span&gt;i.e. pyrene&lt;span class="o"&gt;)&lt;/span&gt;, enter it here. Otherwise, just press
                   enter

  --help           Show this message and exit.&lt;/pre&gt;
&lt;/section&gt;
&lt;section id="closing-thoughts"&gt;
&lt;h2&gt;Closing Thoughts&lt;/h2&gt;
&lt;p&gt;I hope this is a useful tutorial for anyone trying to get a CLI to a broader audience. Though most scientists are primarily used to working with a GUI-based UX, it shouldn't take long to evangelize the advantages of using CLIs for particularly tedious graph creation or simple data analysis. These tools certainly exist in the form of Origin/VBA macros, but Python tends to lend itself to much cleaner code in my personal experience. Even better, having Pipx or Docker makes software that is less likely to break down the road or break system installations.&lt;/p&gt;
&lt;/section&gt;
</content><category term="tutorial"></category><category term="Linux"></category><category term="Scientific Python"></category><category term="Docker"></category><category term="Poetry"></category><category term="pipx"></category><category term="chemistry"></category></entry><entry><title>Going Mouseless, or How I Learned to Stop Clicking and Love the Clack</title><link href="http://dalwilliams.com/going-mouseless-or-how-i-learned-to-stop-clicking-and-love-the-clack.html" rel="alternate"></link><published>2020-07-25T00:00:00-05:00</published><updated>2020-07-25T00:00:00-05:00</updated><author><name>Jon Steven Dal Williams</name></author><id>tag:dalwilliams.com,2020-07-25:/going-mouseless-or-how-i-learned-to-stop-clicking-and-love-the-clack.html</id><summary type="html">&lt;p&gt;Jokes about googling aside, it's no secret that writing code involves some pretty heavy browser usage.
Like may people, I used to be one of those who opened so many tabs that I could no longer see the title on each.
Though there are browser extensions that help with this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Jokes about googling aside, it's no secret that writing code involves some pretty heavy browser usage.
Like may people, I used to be one of those who opened so many tabs that I could no longer see the title on each.
Though there are browser extensions that help with this, I knew there had to be a better way.
This, in a very roundabout way, ended up being the starting point in the journey I'm going to outline in this post.&lt;/p&gt;
&lt;p&gt;On my home home computer, I'm a bit spoiled.
I have a mechanical keyboard, two monitors (one horizontal &amp;amp; one vertical), and a gaming mouse with side buttons.
Because of this, I've always preferred doing any heavy-duty developing on my desktop, and saving minor edits/documentation/model training on AWS for my laptop.
However, for various reasons, I've needed to use my laptop more as of late.
I thus wanted to make my development environment as comfy and efficient as possible.
This was done in four ways:&lt;/p&gt;
&lt;p&gt;1. &lt;strong&gt;Tiling Window Management&lt;/strong&gt; Though this was the first part of my journey, it actually may be the least important, as it requires a decent amount of overhead to set up.
That being said, I definitely believe it was worth it.
Once you get more comfortable with workspace navigation, switching windows, and custom sizing/placement, it's kind of hard to go back.
But, be very warned that &lt;cite&gt;ricing &amp;lt;https://rizonrice.club/Main_Page&amp;gt;__&lt;/cite&gt; your own custom workspace is both very fun and time consuming, so prepare to get sucked down that rabbit hole.
I started with the most popular and apparently easiest-to-use one, &lt;a href="https://i3wm.org"&gt;i3&lt;/a&gt;, though there are many more available.
AwesomeWM is apparently also beginner friendly, and there are some &lt;a href="https://github.com/elenapan/dotfiles/wiki/Gallery"&gt;absolutely gorgeous&lt;/a&gt; themes you can use for inspiration.
However, I found the documentation for i3 to be superior, which means it's certainly my reccomendation.&lt;/p&gt;
&lt;p&gt;2. &lt;strong&gt;Browser choices&lt;/strong&gt; I've used the Chromium-based Brave for quite awhile, and although I highly approve of their mission statement, I wanted something that was both easier to sync across multiple machines, and far more customizable.
This lead me to another Chromium-based browser called Vivaldi with a dizzying number of customizations.
This includes, very importantly, tab stacking and custom hotkeys.
Thus far, I've had consistently less ram usage and less crashes than Brave, though a similar number of compatibility issues.
The ability to add &lt;a href="https://vivaldi.com/blog/5-web-panels-to-add-for-programmers/"&gt;web panels&lt;/a&gt; is also quite nice, though I've had issues with notification badges and ProtonMail staying logged in.
It turns out the hotkey customizations, while super nice, where mostly irrelevant due to a certain extension.&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;Vimium&lt;/strong&gt; Not to sound clickbaity, but this is easily my all-time favorite browser extension (minus an ad-blocker, which is thankfully a default in Vivaldi). I had never used Vim before, but this seemed to be unanimously recommended for making the browser more keyboard-friendly. This gif illustrates my favorite of its features, accessed by simply pressing the &lt;cite&gt;f&lt;/cite&gt; key (or &lt;cite&gt;F&lt;/cite&gt; to open the link in a new tab):&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="image0" src="https://sudipbhandari126.github.io/resources/links-vimium.gif" style="width: 100%" /&gt;&lt;/p&gt;
&lt;p&gt;No more scrolling, no more moving the mouse around, just pure touch typing to search and navigate.
It's an absolute godsend.
There are also plenty of other keybindings, many of which are just more efficient versions of standard browser ones.
For example, instead of the standard &lt;cite&gt;ctrl + pgup/pgdown&lt;/cite&gt; to navigate tabs, it's simply &lt;cite&gt;J&lt;/cite&gt; and &lt;cite&gt;K&lt;/cite&gt;, so your hands never have to leave the home keys.
Unsurprisingly, these are adopted straight from Vim, so you can Guess where this goes next...&lt;/p&gt;
&lt;p&gt;4. &lt;strong&gt;Vim (or vim bindings)&lt;/strong&gt; Yes, I've gone full tech hipster here, but hear me out.
If you're already familiar with the extensive key bindings thanks to Vimium, it actually steepens the learning curve for Vim (and yes, by &amp;quot;steepen&amp;quot;, I mean make easier, as opposed to the colloquially common form of the phrase).
There are several very useful features that I wish were in other IDEs, such as &lt;cite&gt;dw&lt;/cite&gt; to delete a word or &lt;cite&gt;3b&lt;/cite&gt; to go back 5 words, or the ability to save markers.
There several great tutorials outside of the &lt;cite&gt;vimtutor&lt;/cite&gt; that comes with vim, including &lt;a href="https://danielmiessler.com/study/vim/"&gt;this one&lt;/a&gt; and this really entertaining guy's &lt;a href="https://www.youtube.com/channel/UC8ENHE5xdFSwx71u3fDH5Xw"&gt;entire YouTube channel&lt;/a&gt;.
I'm not suggesting going fully into Vim, because thankfully other IDEs such as VSCode have plugins for vim bindings.
The text editor part itself is almost secondary.
The modal nature of Vim is what makes it exceptional.
When writing/coding, think of how long is spent editing vs. how long is spent actually writing.
I'm sure you'll agree that editing takes up far more time.
Having a way to efficiently navigate and rewrite sections of text is more helpful than I can possibly articulate.
If you've ever seen a Vim veteran at work, you'll understand what I mean.
So far I'm absolutely loving it. It feels almost like I'm playing StarCraft again, trying to find the most efficient key combos to max my APM.
As a very important added bonus, I feel it's making me better at Linux as a whole, considering that the vi normal mode keybindings are the &lt;em&gt;lingua franca&lt;/em&gt; of many Linux apps.&lt;/p&gt;
&lt;p&gt;So there you have it.
How having too many tabs led me on a journey to Vim.
It definitely takes awhile to learn, but I think it's been worth it.
Did it make me more efficient, and a faster developer? Time will tell that, but I'll leave you with this very relevant XKCD on the subject.&lt;/p&gt;
&lt;p&gt;&lt;img alt="image1" src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png" style="width: 100%" /&gt;&lt;/p&gt;
</content><category term="tutorial"></category><category term="Linux"></category><category term="Vim"></category><category term="Productivity"></category></entry><entry><title>Migrating from Keras to PyTorch in 2020</title><link href="http://dalwilliams.com/migrating-from-keras-to-pytorch-in-2020.html" rel="alternate"></link><published>2020-05-05T00:00:00-05:00</published><updated>2020-05-05T00:00:00-05:00</updated><author><name>Jon Steven Dal Williams</name></author><id>tag:dalwilliams.com,2020-05-05:/migrating-from-keras-to-pytorch-in-2020.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Note: this specifically applies to PyTorch 1.5 and tf.Keras 2.1.0. As
long as the final number in the version is the same, this should still
be applicable, but otherwise, YMMV&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While completing my fellowship at &lt;a href="insightdatascience.com"&gt;Insight&lt;/a&gt;,
I reached a point in my &lt;a href="github.com/dendrondal/CIf3R"&gt;recyclable classification
project&lt;/a&gt; where …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;Note: this specifically applies to PyTorch 1.5 and tf.Keras 2.1.0. As
long as the final number in the version is the same, this should still
be applicable, but otherwise, YMMV&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While completing my fellowship at &lt;a href="insightdatascience.com"&gt;Insight&lt;/a&gt;,
I reached a point in my &lt;a href="github.com/dendrondal/CIf3R"&gt;recyclable classification
project&lt;/a&gt; where I wanted to implement a
&lt;a href="https://sorenbouma.github.io/blog/oneshot/"&gt;siamese network&lt;/a&gt; to improve the generalizability of my model,
while requiring less training data. The ad-hoc siamese networks I
created in Keras had acceptable results, but were certainly not
production-ready. I began looking on &lt;a href="paperswithcode.com"&gt;papers with code&lt;/a&gt; for a more
SOTA model, and realized that Keras implementations are few and far
between. In fact, PyTorch seems to have &lt;a href="https://paperswithcode.com/trends"&gt;exploded in popularity&lt;/a&gt; to
the point where it is now the #1 framework on papers with code. With
this in mind, I decided it was time to start learning PyTorch.&lt;/p&gt;
&lt;section id="why-move"&gt;
&lt;h2&gt;Why move?&lt;/h2&gt;
&lt;p&gt;In addition to the predominance of PyTorch on paperswithcode, there are
several advantages it offers. Back when I began learning Keras,
TensorFlow was the dominant framework partially due to easier
productionalization of models using tf.Serve. However, since then, the
productionalization gap &lt;a href="https://engineering.fb.com/ai-research/announcing-pytorch-1-0-for-both-research-and-production/"&gt;appears to be
closing&lt;/a&gt;.
In addition, PyTorch comes with out-of-the-box asynchronous behavior
when loading, whereas this still has to be specified and configured when
using tf.Datasets. TensorBoard was, and still is, the predominant
monitoring solution for model training, but it’s now fully supported by
PyTorch. Finally, in my opinion, TensorFlow has made some rather strange
api decisions, whereas PyTorch seems to bear more similarity to Numpy.&lt;/p&gt;
&lt;aside class="m-block m-primary"&gt;
&lt;h3&gt;I would highly reccomend checking out the &lt;a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html"&gt;60 minute blitz&lt;/a&gt; for
a grasp of some of the core concepts of PyTorch. This article serves as
a high-level overview for migration from Keras&lt;/h3&gt;
&lt;/aside&gt;
&lt;/section&gt;
&lt;section id="defining-the-computation-graph"&gt;
&lt;h2&gt;Defining the computation graph&lt;/h2&gt;
&lt;p&gt;Since Keras has both the sequential and functional api, it’s worth doing
a comparison of both with the preferred way of instantiating a PyTorch
network. Here, let’s use a &lt;a href="https://www.google.com/books/edition/Deep_Learning_with_Python/Yo3CAQAACAAJ?hl=en"&gt;simple feedforward
MLP&lt;/a&gt;
as an example:&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;#Keras sequential api&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,)))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;#Keras functional api&lt;/span&gt;
&lt;span class="n"&gt;input_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;#Pytorch example&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;The obvious difference here is that the standard practice for PyTorch
models is encapsulation into a class, and instantiating the parent
&lt;code&gt;Module&lt;/code&gt; class. Another less obvious difference is that both the input
and output sizes of each layer have to be specified in PyTorch. Other
than that, it’s a pretty straightforward difference. Let’s try a less
trivial example that’s more suited to the Keras functional API: a
siamese network for image similarity classification, based on the
&lt;a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"&gt;original paper&lt;/a&gt; with several convolutional layers taken out for
the sake of length. This employs custom layers, shared weights, and
gives an overview of how a multi-input, single output neural network can
be implemented in the two frameworks.&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;#Keras example&lt;/span&gt;
&lt;span class="n"&gt;input_shape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;105&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;105&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;img_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;left_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;right_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Input&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;img_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MaxPooling2D&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Conv2D&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;relu&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;()(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sigmoid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;twin&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get the image embeddings, sharing weights between the two networks&lt;/span&gt;
&lt;span class="n"&gt;encoded_l&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;twin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;left_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;encoded_r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;twin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;right_input&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# merge two encoded inputs with the l1 distance between them&lt;/span&gt;
&lt;span class="n"&gt;L1_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Lambda&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="n"&gt;L1_distance&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;L1_layer&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;encoded_l&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoded_r&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;prediction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;L1_distance&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;siamese_net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;left_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right_input&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;prediction&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# The same model in PyTorch    correct = 0&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;SiameseNetwork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MaxPool2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;64&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;128&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Flatten&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2048&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sigmoid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;Unlike Keras, convolutional layers in PyTorch have arguments in the
order of
&lt;code&gt;in_channel size, out_channels size, kernel_size, stride, padding&lt;/code&gt;,
with the default stride and padding being 1 and 0, respectively. You’re
probably noticing that with the PyTorch model, we stopped around the
&lt;code&gt;twin&lt;/code&gt; definition in the Keras model. The reason being is that the
biggest difference between keras and pytorch is how you train the model,
aka the training loop.&lt;/p&gt;
&lt;/section&gt;
&lt;section id="training-the-model"&gt;
&lt;h2&gt;Training the model&lt;/h2&gt;
&lt;p&gt;Defining the model isn’t very different between Keras and PyTorch, but
training the model certainly is. Rather than calling
&lt;code&gt;model.compile()&lt;/code&gt;, you instead define your forward pass as a method of
your model. Also, your loss function, optimizer, and learning rate are
usually defined in the training loop. Let’s start with the forward pass
and training loop for our first MLP:&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;# Defining the forward pass. Note that this is a method of Model&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# F is an alias for torch.nn.functional&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dense2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;relu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;out&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;This shows two methods of model creation: for the siamese model, we
define the entire model intially, making the forward pass as simple as
&lt;code&gt;return twin(x)&lt;/code&gt;. With the MLP, we defined the layers individually.
Which method is better definitely depends on your use case, but my
intuition is that a neural network that can be drawn as a linear
progression of layers lends itself well to the MLP method, whereas
defining your entire model as an attribute works well for more advanced
graphs such as ResNet/Inception type models, or models with multimodal
input/outputs. Alright, so we have our model and how our data flows
through it. The next step is training and evaluation. This is indeed far
more code than is needed by using &lt;code&gt;callbacks&lt;/code&gt; in Keras, but the
training loop adds complexity in exchange for significantly more
flexibility.&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;# train_loader is some predefined Torch DataLoader instance&lt;/span&gt;
&lt;span class="c1"&gt;# device is your cpu/gpu name&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Assuming X and y are torch tensors, you can also just call X.cuda() instead if&lt;/span&gt;
        &lt;span class="c1"&gt;# you know you don&amp;#39;t need to switch devices.&lt;/span&gt;
        &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# We re-instantiate the gradients during each iteration&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mse_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# Now we back-propagate&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="c1"&gt;# Report accuracy every 10 batches&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Loss of &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; after &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;batch&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt; epochs on training set&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;The function above is meant to be used in a &lt;code&gt;for&lt;/code&gt; loop with a preset
number of epochs. Optimizers are called in a similar manner compared to
Keras. Similar to the LearningRateScheduler in Keras’ callbacks, we now
have several built-in &lt;a href="https://pytorch.org/docs/stable/optim.html?highlight=scheduler#torch.optim.lr_scheduler.StepLR"&gt;adaptive learning
rates&lt;/a&gt;.
We have our training function, now for the test one:&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;# Again, test_loader is a DataLoader instance&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="c1"&gt;# We don&amp;#39;t want to change the gradients, so we freeze the model here&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;F&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mse_loss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sum&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# For binary classification&lt;/span&gt;
            &lt;span class="c1"&gt;# For multiclass, pass keepdim=True above&lt;/span&gt;
            &lt;span class="c1"&gt;# Now we format the actual target and compare it to the predicted one&lt;/span&gt;
            &lt;span class="n"&gt;correct&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;view_as&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Average loss: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s1"&gt;Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;correct&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;Now we have our model with its foward propagation method, a training
function, and a testing function. We presume there is a data loading
function in there somewhere as well. So the final step is putting it all
together, either in script for or in a &lt;code&gt;main&lt;/code&gt; function for CLI
execution. Here is the last bit in script form:&lt;/p&gt;
&lt;pre class="m-code"&gt;&lt;span class="c1"&gt;# With Torch, we have to specify GPU/CPU computation&lt;/span&gt;
&lt;span class="n"&gt;use_cuda&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cuda:0&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;use_cuda&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# First we load the model onto the GPU&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Now we load our optimizer&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Let&amp;#39;s also apply a learning rate decay&lt;/span&gt;
&lt;span class="n"&gt;scheduler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lr_scheduler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StepLR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Now let&amp;#39;s train for 100 epochs&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;train_loader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_loader&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="n"&gt;scheduler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="c1"&gt;#Saving the weights of the model to a pickle file&lt;/span&gt;
&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;torch_example.pt&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;/pre&gt;
&lt;p&gt;Whew, that’s a lot of code for a 3 layer MLP! Of course, this is only a
starting point. You’ll probably want some kind of early stopping
mechanism, monitoring with tensorboard or custom visualizations, a tqdm
progress bar, and/or logging. In performing this excercise, I’m of the
mind that the additional code is actually a good thing, as debugging
becomes far easier as you can isolate the line causing the issue with a
visual debugger (&lt;em&gt;cough&lt;/em&gt; or a print statement &lt;em&gt;cough&lt;/em&gt;), as opposed to
Keras abstracting that complexity away.&lt;/p&gt;
&lt;p&gt;So this post doesn’t get too long, I’m going to direct you to &lt;a href="https://github.com/dendrondal/CIf3R"&gt;the
repository for my Insight project&lt;/a&gt; if you want to see the siamese
network in PyTorch. Overall, PyTorch is pretty great, and a smoother
transition than I originally thought. I’ll have to see if this is just a
honeymoon phase, but I figure there’s likely a reason there are so many
converts as of late. Happy hacking, and thanks for reading!&lt;/p&gt;
&lt;/section&gt;
</content><category term="Tutorial"></category><category term="Keras"></category><category term="PyTorch"></category><category term="TensorFlow"></category><category term="CNNs"></category><category term="computer vision"></category><category term="tutorials"></category></entry></feed>